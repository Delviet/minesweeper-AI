{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "from scipy.ndimage import convolve\n",
    "from env import Game\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "PLAYS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {'*':0,\n",
    "#          '':1,\n",
    "         '0':2,\n",
    "         '1':3,\n",
    "         '2':4,\n",
    "         '3':5,\n",
    "         '4':6,\n",
    "         '5':7,\n",
    "         '6':8,\n",
    "         '7':9,\n",
    "         '8':10}\n",
    "\n",
    "def prepare_view(state):\n",
    "    n = np.zeros((*state.shape, 11))\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            if col:\n",
    "                n[x, y, types[col]] = 1\n",
    "    return n[:, :, :-1]\n",
    "\n",
    "def prepare_rew(state):\n",
    "    n = np.zeros((*state.shape, 1))\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            n[x, y, 0] = (0.5-col)*2\n",
    "    return n\n",
    "\n",
    "def allowed_points(state):\n",
    "    n = np.zeros(state.shape)\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            if col == '*':\n",
    "                n[x, y] = True\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iteration #0 | aver. reward: 3.72 | Victory count: 0 | Target vals: -2.8\n",
      "Iteration #1 | aver. reward: 5.33 | Victory count: 0 | Target vals: 14.13\n",
      "Iteration #2 | aver. reward: 6.17 | Victory count: 0 | Target vals: 23.82\n",
      "Iteration #3 | aver. reward: 6.57 | Victory count: 0 | Target vals: 28.75\n",
      "Iteration #4 | aver. reward: 6.91 | Victory count: 0 | Target vals: 32.8\n",
      "Iteration #5 | aver. reward: 7.24 | Victory count: 1 | Target vals: 37.55\n",
      "Iteration #6 | aver. reward: 7.46 | Victory count: 3 | Target vals: 40.23\n",
      "Iteration #7 | aver. reward: 7.67 | Victory count: 4 | Target vals: 42.81\n",
      "Iteration #8 | aver. reward: 7.96 | Victory count: 6 | Target vals: 47.01\n",
      "Iteration #9 | aver. reward: 8.17 | Victory count: 14 | Target vals: 50.2\n",
      "Iteration #10 | aver. reward: 8.46 | Victory count: 21 | Target vals: 54.76\n",
      "Iteration #11 | aver. reward: 8.74 | Victory count: 24 | Target vals: 58.82\n",
      "Iteration #12 | aver. reward: 8.8 | Victory count: 34 | Target vals: 60.06\n",
      "Iteration #13 | aver. reward: 8.93 | Victory count: 47 | Target vals: 62.06\n",
      "Iteration #14 | aver. reward: 8.99 | Victory count: 38 | Target vals: 62.89\n",
      "Iteration #15 | aver. reward: 8.88 | Victory count: 54 | Target vals: 61.41\n",
      "Iteration #16 | aver. reward: 9.09 | Victory count: 58 | Target vals: 64.31\n",
      "Iteration #17 | aver. reward: 9.09 | Victory count: 73 | Target vals: 64.15\n",
      "Iteration #18 | aver. reward: 9.15 | Victory count: 77 | Target vals: 65.47\n",
      "Iteration #19 | aver. reward: 9.16 | Victory count: 78 | Target vals: 65.55\n",
      "Iteration #20 | aver. reward: 9.29 | Victory count: 120 | Target vals: 67.33\n",
      "Iteration #21 | aver. reward: 9.34 | Victory count: 138 | Target vals: 68.7\n",
      "Iteration #22 | aver. reward: 9.22 | Victory count: 132 | Target vals: 66.39\n",
      "Iteration #23 | aver. reward: 9.41 | Victory count: 165 | Target vals: 69.66\n",
      "Iteration #24 | aver. reward: 9.44 | Victory count: 194 | Target vals: 70.29\n",
      "Iteration #25 | aver. reward: 9.65 | Victory count: 218 | Target vals: 72.82\n",
      "Iteration #26 | aver. reward: 9.57 | Victory count: 249 | Target vals: 72.12\n",
      "Iteration #27 | aver. reward: 9.62 | Victory count: 291 | Target vals: 72.59\n",
      "Iteration #28 | aver. reward: 9.54 | Victory count: 323 | Target vals: 71.49\n",
      "Iteration #29 | aver. reward: 9.53 | Victory count: 339 | Target vals: 71.17\n",
      "Iteration #30 | aver. reward: 9.77 | Victory count: 408 | Target vals: 74.78\n",
      "Iteration #31 | aver. reward: 9.78 | Victory count: 400 | Target vals: 74.69\n",
      "Iteration #32 | aver. reward: 9.67 | Victory count: 429 | Target vals: 73.42\n",
      "Iteration #33 | aver. reward: 9.68 | Victory count: 425 | Target vals: 73.14\n",
      "Iteration #34 | aver. reward: 9.83 | Victory count: 473 | Target vals: 75.83\n",
      "Iteration #35 | aver. reward: 9.7 | Victory count: 485 | Target vals: 73.66\n",
      "Iteration #36 | aver. reward: 9.82 | Victory count: 551 | Target vals: 75.24\n",
      "Iteration #37 | aver. reward: 9.65 | Victory count: 533 | Target vals: 72.59\n",
      "Iteration #38 | aver. reward: 9.64 | Victory count: 558 | Target vals: 72.69\n",
      "Iteration #39 | aver. reward: 9.76 | Victory count: 541 | Target vals: 74.22\n",
      "Iteration #40 | aver. reward: 10.01 | Victory count: 653 | Target vals: 78.45\n",
      "Iteration #41 | aver. reward: 9.9 | Victory count: 584 | Target vals: 76.36\n",
      "Iteration #42 | aver. reward: 10.12 | Victory count: 704 | Target vals: 79.73\n",
      "Iteration #43 | aver. reward: 10.06 | Victory count: 651 | Target vals: 79.21\n",
      "Iteration #44 | aver. reward: 10.34 | Victory count: 732 | Target vals: 83.11\n",
      "Iteration #45 | aver. reward: 10.24 | Victory count: 761 | Target vals: 82.08\n",
      "Iteration #46 | aver. reward: 10.44 | Victory count: 793 | Target vals: 85.01\n",
      "Iteration #47 | aver. reward: 10.29 | Victory count: 779 | Target vals: 82.49\n",
      "Iteration #48 | aver. reward: 10.49 | Victory count: 844 | Target vals: 85.9\n",
      "Iteration #49 | aver. reward: 10.51 | Victory count: 822 | Target vals: 86.1\n",
      "Iteration #50 | aver. reward: 10.82 | Victory count: 969 | Target vals: 91.26\n",
      "Iteration #51 | aver. reward: 10.68 | Victory count: 949 | Target vals: 88.94\n",
      "Iteration #52 | aver. reward: 10.9 | Victory count: 972 | Target vals: 92.46\n",
      "Iteration #53 | aver. reward: 10.93 | Victory count: 981 | Target vals: 92.94\n",
      "Iteration #54 | aver. reward: 11.07 | Victory count: 1072 | Target vals: 95.46\n",
      "Iteration #55 | aver. reward: 11.04 | Victory count: 1101 | Target vals: 94.62\n",
      "Iteration #56 | aver. reward: 11.22 | Victory count: 1106 | Target vals: 98.37\n",
      "Iteration #57 | aver. reward: 11.26 | Victory count: 1109 | Target vals: 98.68\n",
      "Iteration #58 | aver. reward: 11.42 | Victory count: 1229 | Target vals: 101.41\n",
      "Iteration #59 | aver. reward: 11.44 | Victory count: 1210 | Target vals: 101.85\n",
      "Iteration #60 | aver. reward: 11.52 | Victory count: 1248 | Target vals: 103.22\n",
      "Iteration #61 | aver. reward: 11.59 | Victory count: 1334 | Target vals: 104.59\n",
      "Iteration #62 | aver. reward: 11.76 | Victory count: 1385 | Target vals: 107.54\n",
      "Iteration #63 | aver. reward: 11.84 | Victory count: 1427 | Target vals: 108.89\n",
      "Iteration #64 | aver. reward: 11.95 | Victory count: 1493 | Target vals: 110.98\n",
      "Iteration #65 | aver. reward: 11.99 | Victory count: 1528 | Target vals: 111.37\n",
      "Iteration #66 | aver. reward: 11.99 | Victory count: 1571 | Target vals: 111.6\n",
      "Iteration #67 | aver. reward: 12.07 | Victory count: 1604 | Target vals: 113.16\n",
      "Iteration #68 | aver. reward: 12.16 | Victory count: 1680 | Target vals: 114.41\n",
      "Iteration #69 | aver. reward: 12.02 | Victory count: 1598 | Target vals: 111.8\n",
      "Iteration #70 | aver. reward: 12.17 | Victory count: 1723 | Target vals: 114.8\n",
      "Iteration #71 | aver. reward: 12.28 | Victory count: 1765 | Target vals: 116.88\n",
      "Iteration #72 | aver. reward: 12.41 | Victory count: 1828 | Target vals: 118.99\n",
      "Iteration #73 | aver. reward: 12.48 | Victory count: 1890 | Target vals: 120.65\n",
      "Iteration #74 | aver. reward: 12.55 | Victory count: 1954 | Target vals: 121.66\n",
      "Iteration #75 | aver. reward: 12.44 | Victory count: 1951 | Target vals: 119.74\n",
      "Iteration #76 | aver. reward: 12.58 | Victory count: 2032 | Target vals: 122.06\n",
      "Iteration #77 | aver. reward: 12.58 | Victory count: 2030 | Target vals: 122.0\n",
      "Iteration #78 | aver. reward: 12.7 | Victory count: 2145 | Target vals: 124.26\n",
      "Iteration #79 | aver. reward: 12.69 | Victory count: 2132 | Target vals: 124.16\n",
      "Iteration #80 | aver. reward: 12.77 | Victory count: 2209 | Target vals: 125.51\n",
      "Iteration #81 | aver. reward: 12.81 | Victory count: 2207 | Target vals: 126.3\n",
      "Iteration #82 | aver. reward: 12.81 | Victory count: 2176 | Target vals: 126.31\n",
      "Iteration #83 | aver. reward: 12.95 | Victory count: 2351 | Target vals: 128.77\n",
      "Iteration #84 | aver. reward: 12.93 | Victory count: 2354 | Target vals: 128.06\n",
      "Iteration #85 | aver. reward: 12.91 | Victory count: 2306 | Target vals: 127.55\n",
      "Iteration #86 | aver. reward: 13.03 | Victory count: 2357 | Target vals: 130.7\n",
      "Iteration #87 | aver. reward: 12.96 | Victory count: 2316 | Target vals: 128.96\n",
      "Iteration #88 | aver. reward: 12.88 | Victory count: 2297 | Target vals: 127.79\n",
      "Iteration #89 | aver. reward: 12.97 | Victory count: 2388 | Target vals: 128.38\n",
      "Iteration #90 | aver. reward: 12.98 | Victory count: 2345 | Target vals: 129.56\n",
      "Iteration #91 | aver. reward: 13.14 | Victory count: 2452 | Target vals: 132.48\n",
      "Iteration #92 | aver. reward: 13.14 | Victory count: 2520 | Target vals: 131.86\n",
      "Iteration #93 | aver. reward: 13.16 | Victory count: 2440 | Target vals: 132.88\n",
      "Iteration #94 | aver. reward: 13.21 | Victory count: 2437 | Target vals: 133.78\n",
      "Iteration #95 | aver. reward: 13.15 | Victory count: 2461 | Target vals: 132.47\n",
      "Iteration #96 | aver. reward: 13.17 | Victory count: 2446 | Target vals: 132.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #97 | aver. reward: 13.15 | Victory count: 2452 | Target vals: 132.39\n",
      "Iteration #98 | aver. reward: 13.17 | Victory count: 2534 | Target vals: 132.66\n",
      "Iteration #99 | aver. reward: 13.26 | Victory count: 2479 | Target vals: 134.61\n",
      "Iteration #100 | aver. reward: 13.29 | Victory count: 2523 | Target vals: 135.26\n",
      "Iteration #101 | aver. reward: 13.29 | Victory count: 2545 | Target vals: 134.87\n",
      "Iteration #102 | aver. reward: 13.34 | Victory count: 2603 | Target vals: 136.13\n",
      "Iteration #103 | aver. reward: 13.23 | Victory count: 2463 | Target vals: 133.19\n",
      "Iteration #104 | aver. reward: 13.39 | Victory count: 2537 | Target vals: 136.49\n",
      "Iteration #105 | aver. reward: 13.39 | Victory count: 2597 | Target vals: 136.33\n",
      "Iteration #106 | aver. reward: 13.24 | Victory count: 2513 | Target vals: 133.75\n",
      "Iteration #107 | aver. reward: 13.1 | Victory count: 2493 | Target vals: 131.1\n",
      "Iteration #108 | aver. reward: 13.08 | Victory count: 2513 | Target vals: 130.75\n",
      "Iteration #109 | aver. reward: 13.29 | Victory count: 2581 | Target vals: 134.26\n",
      "Iteration #110 | aver. reward: 13.29 | Victory count: 2661 | Target vals: 133.75\n",
      "Iteration #111 | aver. reward: 13.32 | Victory count: 2627 | Target vals: 134.84\n",
      "Iteration #112 | aver. reward: 13.31 | Victory count: 2568 | Target vals: 135.33\n",
      "Iteration #113 | aver. reward: 13.23 | Victory count: 2648 | Target vals: 133.21\n",
      "Iteration #114 | aver. reward: 13.35 | Victory count: 2701 | Target vals: 135.29\n",
      "Iteration #115 | aver. reward: 13.46 | Victory count: 2723 | Target vals: 137.15\n",
      "Iteration #116 | aver. reward: 13.32 | Victory count: 2548 | Target vals: 135.13\n",
      "Iteration #117 | aver. reward: 13.28 | Victory count: 2597 | Target vals: 134.52\n",
      "Iteration #118 | aver. reward: 13.17 | Victory count: 2549 | Target vals: 132.63\n",
      "Iteration #119 | aver. reward: 13.53 | Victory count: 2727 | Target vals: 139.28\n",
      "Iteration #120 | aver. reward: 13.28 | Victory count: 2713 | Target vals: 134.2\n",
      "Iteration #121 | aver. reward: 13.3 | Victory count: 2607 | Target vals: 134.62\n",
      "Iteration #122 | aver. reward: 13.24 | Victory count: 2532 | Target vals: 133.6\n",
      "Iteration #123 | aver. reward: 13.57 | Victory count: 2798 | Target vals: 139.06\n",
      "Iteration #124 | aver. reward: 13.41 | Victory count: 2696 | Target vals: 136.8\n",
      "Iteration #125 | aver. reward: 13.46 | Victory count: 2760 | Target vals: 137.81\n",
      "Iteration #126 | aver. reward: 13.38 | Victory count: 2700 | Target vals: 135.94\n",
      "Iteration #127 | aver. reward: 13.61 | Victory count: 2821 | Target vals: 140.0\n",
      "Iteration #128 | aver. reward: 13.4 | Victory count: 2709 | Target vals: 136.69\n",
      "Iteration #129 | aver. reward: 13.42 | Victory count: 2741 | Target vals: 136.39\n",
      "Iteration #130 | aver. reward: 13.58 | Victory count: 2873 | Target vals: 139.44\n",
      "Iteration #131 | aver. reward: 13.32 | Victory count: 2601 | Target vals: 134.68\n",
      "Iteration #132 | aver. reward: 13.38 | Victory count: 2744 | Target vals: 135.72\n",
      "Iteration #133 | aver. reward: 13.41 | Victory count: 2710 | Target vals: 136.8\n",
      "Iteration #134 | aver. reward: 13.31 | Victory count: 2596 | Target vals: 134.34\n",
      "Iteration #135 | aver. reward: 13.48 | Victory count: 2829 | Target vals: 136.85\n",
      "Iteration #136 | aver. reward: 13.45 | Victory count: 2744 | Target vals: 136.58\n",
      "Iteration #137 | aver. reward: 13.32 | Victory count: 2693 | Target vals: 134.44\n",
      "Iteration #138 | aver. reward: 13.23 | Victory count: 2619 | Target vals: 132.76\n",
      "Iteration #139 | aver. reward: 13.55 | Victory count: 2849 | Target vals: 138.17\n",
      "Iteration #140 | aver. reward: 13.45 | Victory count: 2799 | Target vals: 137.22\n",
      "Iteration #141 | aver. reward: 13.5 | Victory count: 2822 | Target vals: 137.22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f84d9070b95f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_runned\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0minps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprepare_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mest_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mstate_inp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minps\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mest_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mallowed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallowed_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_field\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def agent():\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, 5, 10))\n",
    "    flinp = tf.keras.layers.Flatten()(inp)\n",
    "    d1 = tf.keras.layers.Dense(1250, activation='relu')(flinp)\n",
    "    d2 = tf.keras.layers.Dense(250, activation='relu')(d1)\n",
    "    d3 = tf.keras.layers.Dense(100, activation='relu')(d2)\n",
    "    d4 = tf.keras.layers.Dense(25, activation='relu')(d3)\n",
    "    \n",
    "    x1 = tf.keras.layers.MaxPooling2D((2, 2))(inp)\n",
    "    x2 = tf.keras.layers.AveragePooling2D((2, 2))(inp)\n",
    "    x3 = tf.keras.layers.MaxPooling2D((3, 3))(inp)\n",
    "    x4 = tf.keras.layers.AveragePooling2D((3, 3))(inp)\n",
    "\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x4 = tf.keras.layers.Flatten()(x4)\n",
    "\n",
    "    input_ready = tf.keras.layers.concatenate([x1, x2, x3, x4, flinp, d4])\n",
    "    out = tf.keras.layers.Dense(250)(input_ready)\n",
    "    out = tf.keras.layers.Dense(25)(out)\n",
    "\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    return inp, out\n",
    "\n",
    "    \n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "state_inp, values = agent()\n",
    "\n",
    "reward_phold = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "optim = tf.train.AdamOptimizer()\n",
    "loss_f = tf.reduce_sum(tf.pow(values - reward_phold, 2))\n",
    "train_function = optim.minimize(loss_f)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "env = Game(9, 9, 0.15)\n",
    "for it in range(1000):\n",
    "    rewsum_l = []\n",
    "    targets = []\n",
    "    vics = 0\n",
    "    states  = []\n",
    "    rewards = []\n",
    "    for pl in range(PLAYS):\n",
    "        env.start(mode='bot')\n",
    "        rewsum = 0\n",
    "        target = 0\n",
    "        steps  = 1\n",
    "        while env.game_runned:\n",
    "            inps = np.array([prepare_view(field) for field in env.show()])\n",
    "            est_values = sess.run(values, feed_dict={state_inp:inps})\n",
    "            est_values = np.squeeze(est_values).reshape(9, 9)\n",
    "            allowed = allowed_points(env.pg.player_field).astype(np.uint8)\n",
    "            allowed_flat = [n1*9+n2 for n1, a in enumerate(allowed) \\\n",
    "                            for n2, b in enumerate(a) if b == 1]\n",
    "            point = allowed_flat[est_values.ravel()[allowed.ravel()==1].argmax()]\n",
    "            env.do_step(point%9, point//9)\n",
    "            reward = est_values\n",
    "            rew = 1 if not env.lose else -1\n",
    "            reward[point//9, point%9] += np.copy(rew)\n",
    "            target += steps if not env.lose else (steps - 81)\n",
    "            rewsum += rew\n",
    "            rewards += [reward[point//9, point%9].reshape(1, 1)]\n",
    "            states += [np.copy(inps[point]).reshape(1, 5, 5, 10)]\n",
    "            steps += 1\n",
    "            vics += 1 if env.won else 0\n",
    "            rewsum_l += [np.copy(rewsum)]\n",
    "            targets += [np.copy(target)]\n",
    "    states = np.asarray(states)\n",
    "    rewards = np.asarray(rewards)\n",
    "    targets = np.array(targets)\n",
    "    rewsum_l = np.array(rewsum_l)\n",
    "    print(f'Iteration #{it}', '| aver. reward:', round(np.mean(rewsum_l[rewsum_l>=0]), 2), '| Victory count:', f'{vics} | Target vals:', \n",
    "         round(np.mean(targets), 2))\n",
    "\n",
    "    for ep in range(10):\n",
    "        pos = np.random.choice(range(states.shape[0]), size=(states.shape[0],), replace=False)\n",
    "        inp_st = states[pos].reshape(-1, 5, 5, 10)\n",
    "        inp_rew = rewards[pos].reshape(-1, 1)\n",
    "        sess.run(train_function, feed_dict = {state_inp: inp_st,\n",
    "                                              reward_phold: inp_rew.reshape(-1, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
