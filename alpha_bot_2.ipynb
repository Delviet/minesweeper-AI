{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "from scipy.ndimage import convolve\n",
    "from env import Game\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "PLAYS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {'*':0,\n",
    "#          '':1,\n",
    "         '0':2,\n",
    "         '1':3,\n",
    "         '2':4,\n",
    "         '3':5,\n",
    "         '4':6,\n",
    "         '5':7,\n",
    "         '6':8,\n",
    "         '7':9,\n",
    "         '8':10}\n",
    "\n",
    "def prepare_view(state):\n",
    "    n = np.zeros((*state.shape, 11))\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            if col:\n",
    "                n[x, y, types[col]] = 1\n",
    "    return n[:, :, :-1]\n",
    "\n",
    "def prepare_rew(state):\n",
    "    n = np.zeros((*state.shape, 1))\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            n[x, y, 0] = (0.5-col)*2\n",
    "    return n\n",
    "\n",
    "def allowed_points(state):\n",
    "    n = np.zeros(state.shape)\n",
    "    for x, row in enumerate(state):\n",
    "        for y, col in enumerate(row):\n",
    "            if col == '*':\n",
    "                n[x, y] = True\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iteration #0 | aver. reward: 2.17 | Victory count: 0 | Target vals: -16.47\n",
      "Iteration #1 | aver. reward: 4.82 | Victory count: 2 | Target vals: 10.68\n",
      "Iteration #2 | aver. reward: 5.35 | Victory count: 6 | Target vals: 16.51\n",
      "Iteration #3 | aver. reward: 5.73 | Victory count: 5 | Target vals: 20.64\n",
      "Iteration #4 | aver. reward: 5.95 | Victory count: 8 | Target vals: 23.6\n",
      "Iteration #5 | aver. reward: 6.11 | Victory count: 14 | Target vals: 25.43\n",
      "Iteration #6 | aver. reward: 6.39 | Victory count: 21 | Target vals: 29.06\n",
      "Iteration #7 | aver. reward: 6.64 | Victory count: 22 | Target vals: 32.11\n",
      "Iteration #8 | aver. reward: 6.81 | Victory count: 30 | Target vals: 34.38\n",
      "Iteration #9 | aver. reward: 6.93 | Victory count: 28 | Target vals: 35.69\n",
      "Iteration #10 | aver. reward: 7.03 | Victory count: 35 | Target vals: 37.0\n",
      "Iteration #11 | aver. reward: 7.05 | Victory count: 51 | Target vals: 37.14\n",
      "Iteration #12 | aver. reward: 7.22 | Victory count: 57 | Target vals: 39.6\n",
      "Iteration #13 | aver. reward: 7.25 | Victory count: 64 | Target vals: 39.53\n",
      "Iteration #14 | aver. reward: 7.2 | Victory count: 48 | Target vals: 38.94\n",
      "Iteration #15 | aver. reward: 7.39 | Victory count: 73 | Target vals: 41.63\n",
      "Iteration #16 | aver. reward: 7.48 | Victory count: 110 | Target vals: 42.78\n",
      "Iteration #17 | aver. reward: 7.31 | Victory count: 91 | Target vals: 40.88\n",
      "Iteration #18 | aver. reward: 7.63 | Victory count: 168 | Target vals: 44.97\n",
      "Iteration #19 | aver. reward: 7.75 | Victory count: 222 | Target vals: 46.85\n",
      "Iteration #20 | aver. reward: 7.65 | Victory count: 260 | Target vals: 45.66\n",
      "Iteration #21 | aver. reward: 7.72 | Victory count: 329 | Target vals: 46.46\n",
      "Iteration #22 | aver. reward: 7.77 | Victory count: 317 | Target vals: 46.7\n",
      "Iteration #23 | aver. reward: 7.81 | Victory count: 342 | Target vals: 47.42\n",
      "Iteration #24 | aver. reward: 7.88 | Victory count: 277 | Target vals: 48.26\n",
      "Iteration #25 | aver. reward: 7.89 | Victory count: 399 | Target vals: 48.31\n",
      "Iteration #26 | aver. reward: 8.09 | Victory count: 470 | Target vals: 51.32\n",
      "Iteration #27 | aver. reward: 8.1 | Victory count: 462 | Target vals: 51.55\n",
      "Iteration #28 | aver. reward: 7.96 | Victory count: 430 | Target vals: 49.19\n",
      "Iteration #29 | aver. reward: 8.15 | Victory count: 597 | Target vals: 51.81\n",
      "Iteration #30 | aver. reward: 8.12 | Victory count: 600 | Target vals: 51.86\n",
      "Iteration #31 | aver. reward: 8.09 | Victory count: 688 | Target vals: 51.1\n",
      "Iteration #32 | aver. reward: 8.03 | Victory count: 514 | Target vals: 50.06\n",
      "Iteration #33 | aver. reward: 8.37 | Victory count: 740 | Target vals: 55.38\n",
      "Iteration #34 | aver. reward: 8.27 | Victory count: 732 | Target vals: 53.84\n",
      "Iteration #35 | aver. reward: 8.31 | Victory count: 785 | Target vals: 54.11\n",
      "Iteration #36 | aver. reward: 8.24 | Victory count: 732 | Target vals: 53.07\n",
      "Iteration #37 | aver. reward: 7.93 | Victory count: 671 | Target vals: 49.65\n",
      "Iteration #38 | aver. reward: 8.07 | Victory count: 659 | Target vals: 51.53\n",
      "Iteration #39 | aver. reward: 8.27 | Victory count: 815 | Target vals: 54.39\n",
      "Iteration #40 | aver. reward: 8.39 | Victory count: 912 | Target vals: 56.4\n",
      "Iteration #41 | aver. reward: 8.34 | Victory count: 849 | Target vals: 56.04\n",
      "Iteration #42 | aver. reward: 8.35 | Victory count: 895 | Target vals: 55.66\n",
      "Iteration #43 | aver. reward: 8.49 | Victory count: 928 | Target vals: 57.66\n",
      "Iteration #44 | aver. reward: 8.47 | Victory count: 955 | Target vals: 57.26\n",
      "Iteration #45 | aver. reward: 8.52 | Victory count: 938 | Target vals: 57.59\n",
      "Iteration #46 | aver. reward: 8.77 | Victory count: 1201 | Target vals: 60.9\n",
      "Iteration #47 | aver. reward: 8.58 | Victory count: 1066 | Target vals: 57.78\n",
      "Iteration #48 | aver. reward: 8.75 | Victory count: 1122 | Target vals: 60.61\n",
      "Iteration #49 | aver. reward: 8.88 | Victory count: 1164 | Target vals: 62.35\n",
      "Iteration #50 | aver. reward: 8.96 | Victory count: 1309 | Target vals: 63.66\n",
      "Iteration #51 | aver. reward: 8.88 | Victory count: 1253 | Target vals: 62.41\n",
      "Iteration #52 | aver. reward: 9.06 | Victory count: 1386 | Target vals: 65.2\n",
      "Iteration #53 | aver. reward: 9.12 | Victory count: 1447 | Target vals: 66.14\n",
      "Iteration #54 | aver. reward: 9.22 | Victory count: 1517 | Target vals: 67.27\n",
      "Iteration #55 | aver. reward: 9.25 | Victory count: 1493 | Target vals: 67.63\n",
      "Iteration #56 | aver. reward: 9.31 | Victory count: 1587 | Target vals: 68.92\n",
      "Iteration #57 | aver. reward: 9.43 | Victory count: 1621 | Target vals: 70.58\n",
      "Iteration #58 | aver. reward: 9.37 | Victory count: 1659 | Target vals: 69.53\n",
      "Iteration #59 | aver. reward: 9.52 | Victory count: 1683 | Target vals: 71.74\n",
      "Iteration #60 | aver. reward: 9.45 | Victory count: 1729 | Target vals: 70.57\n",
      "Iteration #61 | aver. reward: 9.54 | Victory count: 1729 | Target vals: 72.21\n",
      "Iteration #62 | aver. reward: 9.52 | Victory count: 1726 | Target vals: 71.96\n",
      "Iteration #63 | aver. reward: 9.55 | Victory count: 1751 | Target vals: 71.93\n",
      "Iteration #64 | aver. reward: 9.55 | Victory count: 1793 | Target vals: 71.98\n",
      "Iteration #65 | aver. reward: 9.65 | Victory count: 1735 | Target vals: 73.74\n",
      "Iteration #66 | aver. reward: 9.73 | Victory count: 1887 | Target vals: 75.05\n",
      "Iteration #67 | aver. reward: 9.65 | Victory count: 1877 | Target vals: 73.46\n",
      "Iteration #68 | aver. reward: 9.78 | Victory count: 1955 | Target vals: 75.4\n",
      "Iteration #69 | aver. reward: 9.81 | Victory count: 2001 | Target vals: 76.04\n",
      "Iteration #70 | aver. reward: 9.84 | Victory count: 2018 | Target vals: 76.25\n",
      "Iteration #71 | aver. reward: 9.81 | Victory count: 2002 | Target vals: 76.1\n",
      "Iteration #72 | aver. reward: 9.89 | Victory count: 2079 | Target vals: 77.15\n",
      "Iteration #73 | aver. reward: 9.96 | Victory count: 2070 | Target vals: 78.19\n",
      "Iteration #74 | aver. reward: 9.87 | Victory count: 2033 | Target vals: 76.46\n",
      "Iteration #75 | aver. reward: 9.93 | Victory count: 2157 | Target vals: 77.49\n",
      "Iteration #76 | aver. reward: 9.91 | Victory count: 2122 | Target vals: 77.12\n",
      "Iteration #77 | aver. reward: 10.04 | Victory count: 2161 | Target vals: 79.52\n",
      "Iteration #78 | aver. reward: 9.9 | Victory count: 2142 | Target vals: 77.13\n",
      "Iteration #79 | aver. reward: 9.98 | Victory count: 2190 | Target vals: 78.05\n",
      "Iteration #80 | aver. reward: 9.94 | Victory count: 2215 | Target vals: 77.51\n",
      "Iteration #81 | aver. reward: 9.99 | Victory count: 2273 | Target vals: 78.11\n",
      "Iteration #82 | aver. reward: 10.05 | Victory count: 2334 | Target vals: 79.33\n",
      "Iteration #83 | aver. reward: 10.06 | Victory count: 2328 | Target vals: 79.17\n",
      "Iteration #84 | aver. reward: 10.12 | Victory count: 2457 | Target vals: 80.1\n",
      "Iteration #85 | aver. reward: 10.16 | Victory count: 2423 | Target vals: 80.58\n",
      "Iteration #86 | aver. reward: 10.13 | Victory count: 2434 | Target vals: 80.62\n",
      "Iteration #87 | aver. reward: 10.15 | Victory count: 2536 | Target vals: 80.57\n",
      "Iteration #88 | aver. reward: 10.08 | Victory count: 2408 | Target vals: 79.66\n",
      "Iteration #89 | aver. reward: 10.14 | Victory count: 2517 | Target vals: 80.35\n",
      "Iteration #90 | aver. reward: 10.16 | Victory count: 2571 | Target vals: 80.27\n",
      "Iteration #91 | aver. reward: 10.07 | Victory count: 2554 | Target vals: 79.07\n",
      "Iteration #92 | aver. reward: 10.17 | Victory count: 2584 | Target vals: 80.56\n",
      "Iteration #93 | aver. reward: 10.08 | Victory count: 2533 | Target vals: 79.36\n",
      "Iteration #94 | aver. reward: 10.09 | Victory count: 2548 | Target vals: 79.38\n",
      "Iteration #95 | aver. reward: 10.19 | Victory count: 2684 | Target vals: 80.93\n",
      "Iteration #96 | aver. reward: 10.09 | Victory count: 2629 | Target vals: 79.36\n",
      "Iteration #97 | aver. reward: 10.11 | Victory count: 2672 | Target vals: 79.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #98 | aver. reward: 10.07 | Victory count: 2645 | Target vals: 79.15\n",
      "Iteration #99 | aver. reward: 10.1 | Victory count: 2672 | Target vals: 79.36\n",
      "Iteration #100 | aver. reward: 10.07 | Victory count: 2660 | Target vals: 79.19\n",
      "Iteration #101 | aver. reward: 10.07 | Victory count: 2703 | Target vals: 78.94\n",
      "Iteration #102 | aver. reward: 10.09 | Victory count: 2675 | Target vals: 79.5\n",
      "Iteration #103 | aver. reward: 10.13 | Victory count: 2737 | Target vals: 80.07\n",
      "Iteration #104 | aver. reward: 10.11 | Victory count: 2723 | Target vals: 79.46\n",
      "Iteration #105 | aver. reward: 10.08 | Victory count: 2628 | Target vals: 79.44\n",
      "Iteration #106 | aver. reward: 10.07 | Victory count: 2702 | Target vals: 79.18\n",
      "Iteration #107 | aver. reward: 10.03 | Victory count: 2739 | Target vals: 78.35\n",
      "Iteration #108 | aver. reward: 10.16 | Victory count: 2757 | Target vals: 80.62\n",
      "Iteration #109 | aver. reward: 10.07 | Victory count: 2778 | Target vals: 78.99\n",
      "Iteration #110 | aver. reward: 10.08 | Victory count: 2831 | Target vals: 79.09\n",
      "Iteration #111 | aver. reward: 10.14 | Victory count: 2795 | Target vals: 80.34\n",
      "Iteration #112 | aver. reward: 10.14 | Victory count: 2778 | Target vals: 80.14\n",
      "Iteration #113 | aver. reward: 10.05 | Victory count: 2767 | Target vals: 78.63\n",
      "Iteration #114 | aver. reward: 10.13 | Victory count: 2829 | Target vals: 80.31\n",
      "Iteration #115 | aver. reward: 10.07 | Victory count: 2747 | Target vals: 79.11\n",
      "Iteration #116 | aver. reward: 10.09 | Victory count: 2753 | Target vals: 79.37\n",
      "Iteration #117 | aver. reward: 10.07 | Victory count: 2797 | Target vals: 79.18\n",
      "Iteration #118 | aver. reward: 10.13 | Victory count: 2930 | Target vals: 79.84\n",
      "Iteration #119 | aver. reward: 10.13 | Victory count: 2885 | Target vals: 79.75\n",
      "Iteration #120 | aver. reward: 10.06 | Victory count: 2839 | Target vals: 78.92\n",
      "Iteration #121 | aver. reward: 10.12 | Victory count: 2815 | Target vals: 79.8\n",
      "Iteration #122 | aver. reward: 10.11 | Victory count: 2897 | Target vals: 79.63\n",
      "Iteration #123 | aver. reward: 10.07 | Victory count: 2833 | Target vals: 79.16\n",
      "Iteration #124 | aver. reward: 10.12 | Victory count: 2847 | Target vals: 79.77\n",
      "Iteration #125 | aver. reward: 9.99 | Victory count: 2813 | Target vals: 77.93\n",
      "Iteration #126 | aver. reward: 9.99 | Victory count: 2765 | Target vals: 77.63\n",
      "Iteration #127 | aver. reward: 10.1 | Victory count: 2927 | Target vals: 79.35\n",
      "Iteration #128 | aver. reward: 10.03 | Victory count: 2934 | Target vals: 78.46\n",
      "Iteration #129 | aver. reward: 10.05 | Victory count: 2797 | Target vals: 78.73\n",
      "Iteration #130 | aver. reward: 10.0 | Victory count: 2935 | Target vals: 78.06\n",
      "Iteration #131 | aver. reward: 10.03 | Victory count: 2851 | Target vals: 78.53\n",
      "Iteration #132 | aver. reward: 10.01 | Victory count: 2857 | Target vals: 78.07\n",
      "Iteration #133 | aver. reward: 10.05 | Victory count: 2841 | Target vals: 78.69\n",
      "Iteration #134 | aver. reward: 10.01 | Victory count: 2875 | Target vals: 78.23\n",
      "Iteration #135 | aver. reward: 9.99 | Victory count: 2916 | Target vals: 77.8\n",
      "Iteration #136 | aver. reward: 10.03 | Victory count: 2928 | Target vals: 78.6\n",
      "Iteration #137 | aver. reward: 9.99 | Victory count: 2887 | Target vals: 77.67\n",
      "Iteration #138 | aver. reward: 10.03 | Victory count: 2941 | Target vals: 78.42\n",
      "Iteration #139 | aver. reward: 10.09 | Victory count: 2939 | Target vals: 79.55\n",
      "Iteration #140 | aver. reward: 10.03 | Victory count: 2875 | Target vals: 78.39\n",
      "Iteration #141 | aver. reward: 9.98 | Victory count: 2875 | Target vals: 77.75\n",
      "Iteration #142 | aver. reward: 10.05 | Victory count: 2956 | Target vals: 78.91\n",
      "Iteration #143 | aver. reward: 10.01 | Victory count: 2943 | Target vals: 78.08\n",
      "Iteration #144 | aver. reward: 10.07 | Victory count: 2954 | Target vals: 78.93\n",
      "Iteration #145 | aver. reward: 9.97 | Victory count: 2896 | Target vals: 77.39\n",
      "Iteration #146 | aver. reward: 9.98 | Victory count: 2934 | Target vals: 77.52\n",
      "Iteration #147 | aver. reward: 10.11 | Victory count: 2997 | Target vals: 79.62\n",
      "Iteration #148 | aver. reward: 10.03 | Victory count: 3019 | Target vals: 78.45\n",
      "Iteration #149 | aver. reward: 10.01 | Victory count: 3019 | Target vals: 78.12\n",
      "Iteration #150 | aver. reward: 10.16 | Victory count: 3064 | Target vals: 80.71\n",
      "Iteration #151 | aver. reward: 10.12 | Victory count: 2949 | Target vals: 79.93\n",
      "Iteration #152 | aver. reward: 10.06 | Victory count: 2974 | Target vals: 78.96\n",
      "Iteration #153 | aver. reward: 9.96 | Victory count: 2939 | Target vals: 77.17\n",
      "Iteration #154 | aver. reward: 10.06 | Victory count: 2977 | Target vals: 79.02\n",
      "Iteration #155 | aver. reward: 10.0 | Victory count: 2958 | Target vals: 77.9\n",
      "Iteration #156 | aver. reward: 10.06 | Victory count: 2971 | Target vals: 78.68\n",
      "Iteration #157 | aver. reward: 9.99 | Victory count: 3009 | Target vals: 77.73\n",
      "Iteration #158 | aver. reward: 10.09 | Victory count: 3067 | Target vals: 79.18\n",
      "Iteration #159 | aver. reward: 10.06 | Victory count: 3075 | Target vals: 78.67\n",
      "Iteration #160 | aver. reward: 10.08 | Victory count: 2965 | Target vals: 79.19\n",
      "Iteration #161 | aver. reward: 10.12 | Victory count: 2981 | Target vals: 79.66\n",
      "Iteration #162 | aver. reward: 10.1 | Victory count: 3074 | Target vals: 79.43\n",
      "Iteration #163 | aver. reward: 10.13 | Victory count: 3010 | Target vals: 80.25\n",
      "Iteration #164 | aver. reward: 10.14 | Victory count: 3039 | Target vals: 80.34\n",
      "Iteration #165 | aver. reward: 10.13 | Victory count: 3073 | Target vals: 80.13\n",
      "Iteration #166 | aver. reward: 10.2 | Victory count: 3148 | Target vals: 81.12\n",
      "Iteration #167 | aver. reward: 10.15 | Victory count: 3154 | Target vals: 80.37\n",
      "Iteration #168 | aver. reward: 10.09 | Victory count: 3064 | Target vals: 79.69\n",
      "Iteration #169 | aver. reward: 10.14 | Victory count: 3030 | Target vals: 80.14\n",
      "Iteration #170 | aver. reward: 10.05 | Victory count: 3047 | Target vals: 78.87\n",
      "Iteration #171 | aver. reward: 10.13 | Victory count: 3041 | Target vals: 80.19\n",
      "Iteration #172 | aver. reward: 10.06 | Victory count: 3119 | Target vals: 78.84\n",
      "Iteration #173 | aver. reward: 10.14 | Victory count: 3091 | Target vals: 80.23\n",
      "Iteration #174 | aver. reward: 10.2 | Victory count: 3075 | Target vals: 81.33\n",
      "Iteration #175 | aver. reward: 10.24 | Victory count: 3144 | Target vals: 81.79\n",
      "Iteration #176 | aver. reward: 10.17 | Victory count: 3156 | Target vals: 80.6\n",
      "Iteration #177 | aver. reward: 10.27 | Victory count: 3240 | Target vals: 82.3\n",
      "Iteration #178 | aver. reward: 10.28 | Victory count: 3187 | Target vals: 82.69\n",
      "Iteration #179 | aver. reward: 10.18 | Victory count: 3236 | Target vals: 80.79\n",
      "Iteration #180 | aver. reward: 10.3 | Victory count: 3175 | Target vals: 82.69\n",
      "Iteration #181 | aver. reward: 10.08 | Victory count: 3054 | Target vals: 79.36\n",
      "Iteration #182 | aver. reward: 10.18 | Victory count: 3098 | Target vals: 81.03\n",
      "Iteration #183 | aver. reward: 10.28 | Victory count: 3189 | Target vals: 82.32\n",
      "Iteration #184 | aver. reward: 10.35 | Victory count: 3221 | Target vals: 83.51\n",
      "Iteration #185 | aver. reward: 10.25 | Victory count: 3189 | Target vals: 81.74\n",
      "Iteration #186 | aver. reward: 10.33 | Victory count: 3252 | Target vals: 83.14\n",
      "Iteration #187 | aver. reward: 10.26 | Victory count: 3165 | Target vals: 82.08\n",
      "Iteration #188 | aver. reward: 10.26 | Victory count: 3311 | Target vals: 82.08\n",
      "Iteration #189 | aver. reward: 10.26 | Victory count: 3215 | Target vals: 82.01\n",
      "Iteration #190 | aver. reward: 10.29 | Victory count: 3217 | Target vals: 82.81\n",
      "Iteration #191 | aver. reward: 10.32 | Victory count: 3286 | Target vals: 82.97\n",
      "Iteration #192 | aver. reward: 10.38 | Victory count: 3293 | Target vals: 83.84\n",
      "Iteration #193 | aver. reward: 10.24 | Victory count: 3148 | Target vals: 81.81\n",
      "Iteration #194 | aver. reward: 10.2 | Victory count: 3228 | Target vals: 81.08\n",
      "Iteration #195 | aver. reward: 10.39 | Victory count: 3319 | Target vals: 84.39\n",
      "Iteration #196 | aver. reward: 10.27 | Victory count: 3277 | Target vals: 82.14\n",
      "Iteration #197 | aver. reward: 10.35 | Victory count: 3298 | Target vals: 83.69\n",
      "Iteration #198 | aver. reward: 10.31 | Victory count: 3213 | Target vals: 82.51\n",
      "Iteration #199 | aver. reward: 10.2 | Victory count: 3240 | Target vals: 80.96\n",
      "Iteration #200 | aver. reward: 10.39 | Victory count: 3295 | Target vals: 84.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #201 | aver. reward: 10.42 | Victory count: 3284 | Target vals: 84.86\n",
      "Iteration #202 | aver. reward: 10.37 | Victory count: 3293 | Target vals: 83.91\n",
      "Iteration #203 | aver. reward: 10.37 | Victory count: 3287 | Target vals: 83.91\n",
      "Iteration #204 | aver. reward: 10.43 | Victory count: 3349 | Target vals: 84.83\n",
      "Iteration #205 | aver. reward: 10.4 | Victory count: 3336 | Target vals: 84.72\n",
      "Iteration #206 | aver. reward: 10.43 | Victory count: 3412 | Target vals: 85.06\n",
      "Iteration #207 | aver. reward: 10.49 | Victory count: 3361 | Target vals: 85.86\n",
      "Iteration #208 | aver. reward: 10.35 | Victory count: 3389 | Target vals: 83.24\n",
      "Iteration #209 | aver. reward: 10.36 | Victory count: 3397 | Target vals: 83.59\n",
      "Iteration #210 | aver. reward: 10.43 | Victory count: 3426 | Target vals: 84.86\n",
      "Iteration #211 | aver. reward: 10.45 | Victory count: 3419 | Target vals: 84.97\n",
      "Iteration #212 | aver. reward: 10.46 | Victory count: 3387 | Target vals: 85.03\n",
      "Iteration #213 | aver. reward: 10.49 | Victory count: 3442 | Target vals: 85.57\n",
      "Iteration #214 | aver. reward: 10.49 | Victory count: 3455 | Target vals: 85.97\n",
      "Iteration #215 | aver. reward: 10.47 | Victory count: 3434 | Target vals: 85.51\n",
      "Iteration #216 | aver. reward: 10.42 | Victory count: 3463 | Target vals: 84.53\n",
      "Iteration #217 | aver. reward: 10.49 | Victory count: 3447 | Target vals: 85.78\n",
      "Iteration #218 | aver. reward: 10.48 | Victory count: 3169 | Target vals: 86.08\n",
      "Iteration #219 | aver. reward: 10.34 | Victory count: 3088 | Target vals: 84.22\n",
      "Iteration #220 | aver. reward: 10.49 | Victory count: 3126 | Target vals: 86.55\n",
      "Iteration #221 | aver. reward: 10.33 | Victory count: 3043 | Target vals: 83.81\n",
      "Iteration #222 | aver. reward: 10.37 | Victory count: 3089 | Target vals: 84.78\n",
      "Iteration #223 | aver. reward: 10.45 | Victory count: 3090 | Target vals: 86.12\n",
      "Iteration #224 | aver. reward: 10.33 | Victory count: 2992 | Target vals: 83.99\n",
      "Iteration #225 | aver. reward: 10.33 | Victory count: 3101 | Target vals: 83.62\n",
      "Iteration #226 | aver. reward: 10.27 | Victory count: 3018 | Target vals: 83.01\n",
      "Iteration #227 | aver. reward: 10.48 | Victory count: 3132 | Target vals: 86.44\n",
      "Iteration #228 | aver. reward: 10.37 | Victory count: 3045 | Target vals: 84.57\n",
      "Iteration #229 | aver. reward: 10.36 | Victory count: 3068 | Target vals: 84.54\n",
      "Iteration #230 | aver. reward: 10.37 | Victory count: 3062 | Target vals: 84.69\n",
      "Iteration #231 | aver. reward: 10.32 | Victory count: 3086 | Target vals: 83.88\n",
      "Iteration #232 | aver. reward: 10.34 | Victory count: 2998 | Target vals: 84.54\n",
      "Iteration #233 | aver. reward: 10.35 | Victory count: 3073 | Target vals: 84.49\n",
      "Iteration #234 | aver. reward: 10.4 | Victory count: 3129 | Target vals: 84.94\n",
      "Iteration #235 | aver. reward: 10.44 | Victory count: 3147 | Target vals: 85.91\n",
      "Iteration #236 | aver. reward: 10.31 | Victory count: 3004 | Target vals: 83.75\n",
      "Iteration #237 | aver. reward: 10.36 | Victory count: 3101 | Target vals: 84.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-6c841de43af2>\", line 51, in <module>\n",
      "    inps = np.array([prepare_view(field) for field in env.show()])\n",
      "  File \"<ipython-input-3-6c841de43af2>\", line 51, in <listcomp>\n",
      "    inps = np.array([prepare_view(field) for field in env.show()])\n",
      "  File \"<ipython-input-2-0087678d520e>\", line 14, in prepare_view\n",
      "    n = np.zeros((*state.shape, 11))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Daniil\\Anaconda3\\lib\\inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "def agent():\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, 5, 10))\n",
    "    flinp = tf.keras.layers.Flatten()(inp)\n",
    "    d1 = tf.keras.layers.Dense(1250, activation='relu')(flinp)\n",
    "    d2 = tf.keras.layers.Dense(250, activation='relu')(d1)\n",
    "    d3 = tf.keras.layers.Dense(100, activation='relu')(d2)\n",
    "    d4 = tf.keras.layers.Dense(25, activation='relu')(d3)\n",
    "    \n",
    "    x1 = tf.keras.layers.MaxPooling2D((2, 2))(inp)\n",
    "    x2 = tf.keras.layers.AveragePooling2D((2, 2))(inp)\n",
    "    x3 = tf.keras.layers.MaxPooling2D((3, 3))(inp)\n",
    "    x4 = tf.keras.layers.AveragePooling2D((3, 3))(inp)\n",
    "\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x4 = tf.keras.layers.Flatten()(x4)\n",
    "\n",
    "    input_ready = tf.keras.layers.concatenate([x1, x2, x3, x4, flinp, d4])\n",
    "    out = tf.keras.layers.Dense(250)(input_ready)\n",
    "    out = tf.keras.layers.Dense(25)(out)\n",
    "\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    return inp, out\n",
    "\n",
    "    \n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "state_inp, values = agent()\n",
    "\n",
    "reward_phold = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "optim = tf.train.AdamOptimizer()\n",
    "loss_f = tf.reduce_sum(tf.pow(values - reward_phold, 2))\n",
    "train_function = optim.minimize(loss_f)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "env = Game(9, 9, 0.15)\n",
    "for it in range(1000):\n",
    "    rewsum_l = []\n",
    "    targets = []\n",
    "    vics = 0\n",
    "    states  = []\n",
    "    rewards = []\n",
    "    for pl in range(PLAYS):\n",
    "        env.start(mode='bot')\n",
    "        rewsum = 0\n",
    "        target = 0\n",
    "        steps  = 1\n",
    "        while env.game_runned:\n",
    "            inps = np.array([prepare_view(field) for field in env.show()])\n",
    "            est_values = sess.run(values, feed_dict={state_inp:inps})\n",
    "            est_values = np.squeeze(est_values).reshape(9, 9)\n",
    "            allowed = allowed_points(env.pg.player_field).astype(np.uint8)\n",
    "            allowed_flat = [n1*9+n2 for n1, a in enumerate(allowed) \\\n",
    "                            for n2, b in enumerate(a) if b == 1]\n",
    "            point = allowed_flat[est_values.ravel()[allowed.ravel()==1].argmax()]\n",
    "            env.do_step(point%9, point//9)\n",
    "            reward = est_values\n",
    "            rew = 1 if not env.lose else -1\n",
    "            reward[point//9, point%9] += np.copy(rew)\n",
    "            target += steps if not env.lose else (steps - 81)\n",
    "            rewsum += rew\n",
    "            rewards += [reward[point//9, point%9].reshape(1, 1)]\n",
    "            states += [np.copy(inps[point]).reshape(1, 5, 5, 10)]\n",
    "            steps += 1\n",
    "            vics += 1 if env.won else 0\n",
    "            rewsum_l += [np.copy(rewsum)]\n",
    "            targets += [np.copy(target)]\n",
    "    states = np.asarray(states)\n",
    "    rewards = np.asarray(rewards)\n",
    "    targets = np.array(targets)\n",
    "    rewsum_l = np.array(rewsum_l)\n",
    "    print(f'Iteration #{it}', '| aver. reward:', round(np.mean(rewsum_l[rewsum_l>=0]), 2), '| Victory count:', f'{vics} | Target vals:', \n",
    "         round(np.mean(targets), 2))\n",
    "\n",
    "    for ep in range(10):\n",
    "        pos = np.random.choice(range(states.shape[0]), size=(states.shape[0],), replace=False)\n",
    "        inp_st = states[pos].reshape(-1, 5, 5, 10)\n",
    "        inp_rew = rewards[pos].reshape(-1, 1)\n",
    "        sess.run(train_function, feed_dict = {state_inp: inp_st,\n",
    "                                              reward_phold: inp_rew.reshape(-1, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
